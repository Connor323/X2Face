{
 "cells": [
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "This jupyter notebook contains demo code for:\n",
    "- loading a model and using it to \n",
    "- drive one or more source frames with a set of driving frames\n",
    "- modifying the embedded face to perform video editing for both the dragon tattoo and Harry Potter scar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import torch\n",
    "from PIL import Image\n",
    "from torch.autograd import Variable\n",
    "from UnwrappedFace import UnwrappedFaceWeightedAverage, UnwrappedFaceWeightedAveragePose\n",
    "import torchvision\n",
    "from torchvision.transforms import ToTensor, Compose, Scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils\n",
    "utils.seed_everything(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_batch(source_images, pose_images):\n",
    "    return model(pose_images, *source_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "# of videos: 4547\n"
    }
   ],
   "source": [
    "from dataset.voxceleb1_dataset import Voxceleb1Dataset\n",
    "data_reader = Voxceleb1Dataset(dataroot='/pub1/hao66/dataset/voxceleb1/unzippedFaces',\n",
    "                               image_size=256, \n",
    "                               isTrain=False)\n",
    "dataset_size = len(data_reader)    # get the number of images in the dataset.\n",
    "data_loader = torch.utils.data.DataLoader(dataset=data_reader, \n",
    "                                          batch_size=1, \n",
    "                                          shuffle=False)\n",
    "print('# of videos: %d' % len(data_reader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "initialization method [xavier]\ninitialization method [xavier]\n"
    }
   ],
   "source": [
    "BASE_MODEL = './release_models/' # Change to your path\n",
    "state_dict = torch.load(BASE_MODEL + 'x2face_model_forpython3.pth')\n",
    "\n",
    "model = UnwrappedFaceWeightedAverage(output_num_channels=2, input_num_channels=3, inner_nc=128)\n",
    "model.load_state_dict(state_dict['state_dict'])\n",
    "model = model.cuda()\n",
    "\n",
    "model = model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Test loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "100%|██████████| 4547/4547 [02:53<00:00, 26.25it/s]\n"
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "save_path = \"./results\"\n",
    "for driver_images, source_images, generator_gt, query_file_path in tqdm(data_loader):\n",
    "    driver_images = driver_images.cuda()/255\n",
    "    source_images = source_images.cuda()/255\n",
    "    # Run the model for each\n",
    "    with torch.no_grad():\n",
    "        result = run_batch([source_images], driver_images)\n",
    "    result = result.clamp(min=0, max=1)\n",
    "    visuals = {\"embedder_input\": driver_images, \"generator_input\": source_images, \n",
    "               \"generator_gt\": generator_gt,    \"generator_output\": result}\n",
    "        \n",
    "    utils.save_images(visuals, query_file_path[0], save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "**Code for driving with another set of frames**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver_path = './examples/Taylor_Swift/1.6/nuBaabkzzzI/'\n",
    "source_path = './examples/Taylor_Swift/1.6/vBgiDYBCuxY/'\n",
    "\n",
    "driver_imgs = [driver_path + d for d in sorted(os.listdir(driver_path))][0:8] # 8 driving frames\n",
    "source_imgs  = [source_path + d for d in sorted(os.listdir(source_path))][0:3] # 3 source frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_img(file_path):\n",
    "    img = Image.open(file_path)\n",
    "    transform = Compose([Scale((256,256)), ToTensor()])\n",
    "    return Variable(transform(img)).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Driving the source image with the driving sequence\n",
    "source_images = []\n",
    "for img in source_imgs:\n",
    "    source_images.append(load_img(img).unsqueeze(0).repeat(len(driver_imgs), 1, 1, 1))\n",
    "    \n",
    "driver_images = None\n",
    "for img in driver_imgs:\n",
    "    if driver_images is None:\n",
    "        driver_images = load_img(img).unsqueeze(0)\n",
    "    else:\n",
    "        driver_images = torch.cat((driver_images, load_img(img).unsqueeze(0)), 0)\n",
    "print(len(source_images), source_images[0].shape, driver_images.shape)\n",
    "\n",
    "# Run the model for each\n",
    "with torch.no_grad():\n",
    "    result = run_batch(source_images, driver_images)\n",
    "result = result.clamp(min=0, max=1)\n",
    "img = torchvision.utils.make_grid(result.cpu().data)\n",
    "    \n",
    "# Visualise the results\n",
    "fig_size = plt.rcParams[\"figure.figsize\"]\n",
    "fig_size[0] = 24.\n",
    "fig_size[1] = 24.\n",
    "plt.rcParams[\"figure.figsize\"] = fig_size\n",
    "plt.axis('off')\n",
    "\n",
    "result_images = img.permute(1,2,0).numpy()\n",
    "driving_images = torchvision.utils.make_grid(driver_images.cpu().data).permute(1,2,0).numpy()\n",
    "print(\"The results is: \")\n",
    "plt.imshow(np.vstack((result_images, driving_images)))"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "**Code for using the dragon tattoo to modify the unwrapped mosaic**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the dragon tattoo\n",
    "fig_size = plt.rcParams[\"figure.figsize\"]\n",
    "fig_size[0] = 1.\n",
    "fig_size[1] = 1.\n",
    "plt.rcParams[\"figure.figsize\"] = fig_size\n",
    "\n",
    "tattoo = Image.open('./tattoos/dragon_tattoo.png')\n",
    "tattoo = tattoo.resize((30,30), Image.BILINEAR).rotate(45).convert('RGBA')\n",
    "\n",
    "b, g, r, a = tattoo.split()\n",
    "tattoo = Image.merge(\"RGBA\", (g, b, r, a))\n",
    "tattoo_flipped = tattoo.transpose(Image.FLIP_LEFT_RIGHT)\n",
    "\n",
    "tattoo = np.array(tattoo) / 256.\n",
    "tattoo[:,:,[1,2]] = 1\n",
    "tattoo_flipped = np.array(tattoo_flipped) / 256.\n",
    "tattoo_flipped[:,:,[1,2]] = 1\n",
    "print(\"The tattoo: \")\n",
    "plt.imshow(tattoo)\n",
    "\n",
    "# Add a flipped version to make a moustache\n",
    "tattoo = torch.Tensor(tattoo)\n",
    "tattoo_flipped = torch.Tensor(tattoo_flipped)\n",
    "tattoo = tattoo.permute(2,0,1)\n",
    "tattoo_flipped = tattoo_flipped.permute(2,0,1)\n",
    "\n",
    "# Changes the colour\n",
    "alpha = tattoo[3:4,:,:].cuda()\n",
    "tattoo = tattoo[0:3,:,:].cuda()\n",
    "alpha_flipped = tattoo_flipped[3:4,:,:].cuda()\n",
    "tattoo_flipped = tattoo_flipped[0:3,:,:].cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Reload driving / source frames\n",
    "driver_path = './examples/Taylor_Swift/1.6/nuBaabkzzzI/'\n",
    "source_path = './examples/Taylor_Swift/1.6/nuBaabkzzzI/'\n",
    "\n",
    "driver_imgs = [driver_path + d for d in sorted(os.listdir(driver_path))][0:8] # 8 driving frames\n",
    "source_imgs  = [source_path + d for d in sorted(os.listdir(source_path))][0:1] # 1 source frames\n",
    "\n",
    "source_images = []\n",
    "for img in source_imgs:\n",
    "    source_images.append(load_img(img).unsqueeze(0))\n",
    "    \n",
    "driver_images = None\n",
    "for img in driver_imgs:\n",
    "    if driver_images is None:\n",
    "        driver_images = load_img(img).unsqueeze(0)\n",
    "    else:\n",
    "        driver_images = torch.cat((driver_images, load_img(img).unsqueeze(0)), 0)\n",
    "\n",
    "        \n",
    "# Modify the face with the given tattoo \n",
    "fig_size = plt.rcParams[\"figure.figsize\"]\n",
    "fig_size[0] = 4.\n",
    "fig_size[1] = 4.\n",
    "plt.rcParams[\"figure.figsize\"] = fig_size\n",
    "\n",
    "# Get the unwrapped face\n",
    "with torch.no_grad():\n",
    "    sampler = model.pix2pixUnwrapped(*source_images)[0][:,0:2,:,:]\n",
    "    xs = np.linspace(-1,1,256)\n",
    "    xs = np.meshgrid(xs, xs)  \n",
    "    xs = np.stack(xs, 2) \n",
    "    xs = torch.Tensor(xs).unsqueeze(0).repeat(1, 1,1,1).cuda()\n",
    "    sampler = nn.Tanh()(sampler).permute(0,2,3,1) + Variable(xs)\n",
    "\n",
    "# Visualise unwrapped face with tattoo\n",
    "result = nn.functional.grid_sample(source_images[0],  sampler).squeeze().cpu()\n",
    "alpha = alpha.cpu()\n",
    "tattoo = tattoo.cpu()\n",
    "to_copy = tattoo * alpha.float() + (1 - alpha.float()) * result[:,125:155,90:120].data.cpu()\n",
    "to_copy = Variable(to_copy).cuda()\n",
    "result[:,125:155,90:120] = to_copy\n",
    "\n",
    "alpha_flipped = alpha_flipped.cpu()\n",
    "tattoo_flipped = tattoo_flipped.cpu()\n",
    "to_copy = tattoo_flipped * alpha_flipped.float() + (1 - alpha_flipped.float()) * result[:,125:155,150:180].data.cpu()\n",
    "to_copy = Variable(to_copy).cuda()\n",
    "result[:,125:155,150:180] = to_copy\n",
    "plt.imshow(result.data.squeeze().cpu().permute(1,2,0).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the model for each\n",
    "with torch.no_grad():\n",
    "    output_sampler = model.pix2pixSampler(driver_images)[0]\n",
    "    output_sampler = nn.Tanh()(output_sampler).permute(0,2,3,1) + Variable(xs)\n",
    "    result = nn.functional.grid_sample(result.unsqueeze(0).repeat(output_sampler.size(0), 1, 1, 1), \n",
    "                                       output_sampler.cpu()).squeeze()\n",
    "    \n",
    "result = result.clamp(min=0, max=1)\n",
    "img = torchvision.utils.make_grid(result.cpu().data)\n",
    "    \n",
    "# Visualise the results\n",
    "fig_size = plt.rcParams[\"figure.figsize\"]\n",
    "fig_size[0] = 24.\n",
    "fig_size[1] = 24.\n",
    "plt.rcParams[\"figure.figsize\"] = fig_size\n",
    "plt.axis('off')\n",
    "\n",
    "result_images = img.permute(1,2,0).numpy()\n",
    "driving_images = torchvision.utils.make_grid(driver_images.cpu().data).permute(1,2,0).numpy()\n",
    "print(\"The unwrapped mosaic driven with the given face is: \")\n",
    "plt.imshow(np.vstack((result_images, driving_images)))"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "**Code for using the Harry Potter scar to modify the unwrapped mosaic**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generat the tattoo for the hp scar\n",
    "index = 7\n",
    "fig_size = plt.rcParams[\"figure.figsize\"]\n",
    "fig_size[0] = 1.\n",
    "fig_size[1] = 1.\n",
    "plt.rcParams[\"figure.figsize\"] = fig_size\n",
    "from PIL import Image\n",
    "tattoo = Image.open('./tattoos/hp_scar_2.png')\n",
    "tattoo = tattoo.resize((30,30), Image.BILINEAR).rotate(45).convert('RGBA')\n",
    "b, g, r, a = tattoo.split()\n",
    "tattoo = Image.merge(\"RGBA\", (g, b, r, a))\n",
    "\n",
    "tattoo = np.array(tattoo) / 256.\n",
    "tattoo[:,:,[0,1]] = 1\n",
    "tattoo = torch.Tensor(tattoo)\n",
    "tattoo = tattoo.permute(2,0,1)\n",
    "alpha = tattoo[2,:,:] < 0.9\n",
    "tattoo = tattoo[0:3,:,:]\n",
    "\n",
    "# Graph the result concatenated with the original frame across a sequence of frames : does the expression change?\n",
    "# Match the \n",
    "fig_size = plt.rcParams[\"figure.figsize\"]\n",
    "fig_size[0] = 4.\n",
    "fig_size[1] = 4.\n",
    "plt.rcParams[\"figure.figsize\"] = (5,5)\n",
    "\n",
    "# Get the unwrapped face\n",
    "with torch.no_grad():\n",
    "    sampler = model.pix2pixUnwrapped(*source_images)[0][:,0:2,:,:]\n",
    "    xs = np.linspace(-1,1,256)\n",
    "    xs = np.meshgrid(xs, xs)  \n",
    "    xs = np.stack(xs, 2) \n",
    "    xs = torch.Tensor(xs).unsqueeze(0).repeat(1, 1,1,1).cuda()\n",
    "    sampler = nn.Tanh()(sampler).permute(0,2,3,1) + Variable(xs)\n",
    "    result = nn.functional.grid_sample(source_images[0],  sampler).squeeze()\n",
    "\n",
    "alpha = alpha.float().clamp(max=1.0) # modify to make more/less realistic\n",
    "to_copy = tattoo * alpha.float() + (1 - alpha.float()) * result[:,45:75,120:150].data.cpu()\n",
    "to_copy = Variable(to_copy).cuda()\n",
    "result[:,45:75,120:150] = to_copy\n",
    "plt.imshow(result.data.squeeze().cpu().permute(1,2,0).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the model for each\n",
    "with torch.no_grad():\n",
    "    output_sampler = model.pix2pixSampler(driver_images)[0]\n",
    "    output_sampler = nn.Tanh()(output_sampler).permute(0,2,3,1) + Variable(xs)\n",
    "    result = nn.functional.grid_sample(result.unsqueeze(0).repeat(output_sampler.size(0), 1, 1, 1).cpu(), \n",
    "                                       output_sampler.cpu()).squeeze()\n",
    "    \n",
    "result = result.clamp(min=0, max=1)\n",
    "img = torchvision.utils.make_grid(result.cpu().data)\n",
    "    \n",
    "# Visualise the results\n",
    "fig_size = plt.rcParams[\"figure.figsize\"]\n",
    "fig_size[0] = 24.\n",
    "fig_size[1] = 24.\n",
    "plt.rcParams[\"figure.figsize\"] = fig_size\n",
    "plt.axis('off')\n",
    "\n",
    "result_images = img.permute(1,2,0).numpy()\n",
    "driving_images = torchvision.utils.make_grid(driver_images.cpu().data).permute(1,2,0).numpy()\n",
    "print(\"The unwrapped mosaic driven with the given face is: \")\n",
    "plt.imshow(np.vstack((result_images, driving_images)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}